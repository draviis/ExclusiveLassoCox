#as.numeric(coeffs != 0)
selected_binary <- rep(0, length(coeffs))
non_zero_indices <- which(coeffs != 0)
selected_binary[non_zero_indices] <- 1
true_positives <- sum(true_coefs & selected_binary)
false_positives <- sum(selected_binary & !true_coefs)
false_negatives <- sum(true_coefs & !selected_binary)
true_negatives <- sum(!true_coefs & !selected_binary)
precision <- true_positives / (true_positives + false_positives)
recall <- true_positives / (true_positives + false_negatives)
f1_score <- ifelse((precision + recall) > 0, 2 * (precision * recall) / (precision + recall), 0)
fnr <- false_negatives / (false_negatives + true_positives)
fdr <- false_positives / (false_positives + true_positives)
accuracy <- (true_positives + true_negatives) / length(true_coefs)      # Compute recall
cindex_test <- concordance.index(pred_test, y_test[,1], y_test[,2])$c.index
# Compute Brier Score
surv_train <- exp(-pred_test)  # Default survival probability calculation
surv_validate <- exp(-pred_test)
# Define time points for Brier score calculation
time_points <- quantile(testing_data$time, probs = c(0.25, 0.50, 0.75))
y_predicted_newdata <- as.vector(surv_validate)
# Rename the "status" column to "event"
colnames(training_data)[colnames(training_data) == "status"] <- "event"
colnames(testing_data)[colnames(testing_data) == "status"] <- "event"
# Initialize Brier Score storage
brier_scores <- numeric(length(time_points))
names(brier_scores) <- paste0("Time_", time_points)
# Compute Brier Score
for (j in seq_along(time_points)) {
brier_scores[j] <- survcompare:::surv_brierscore(
y_predicted_newdata = y_predicted_newdata,
df_brier_train = training_data,
df_newdata = testing_data,
time_point = time_points[j],
weighted = FALSE
)
}
# Compute Integrated Brier Score (IBS)
time_points <- sort(time_points)
brier_scores <- na.omit(brier_scores)
time_points <- time_points[!is.na(brier_scores)]
if (length(brier_scores) > 1) {
time_intervals <- diff(time_points)
ibs <- sum(time_intervals * (brier_scores[-length(brier_scores)] + brier_scores[-1]) / 2) /
(max(time_points) - min(time_points))
} else {
ibs <- NA
}
sim_results[[model_name]] <- list(
recall = recall,
brier_scores = brier_scores,
ibs = ibs
)
sim_results[[model_name]] <- list(
recall = recall,
accuracy = accuracy,
f1_score = f1_score,
fdr = fdr,
fnr= fnr,
cindex_test = cindex_test,
brier_scores = brier_scores,
ibs = ibs
)
# print(paste("Model:", model_name))
# print(paste("Recall:", recall))
# print(paste("Accuracy:", accuracy))
# print(paste("F1 Score:", f1_score))
# print(paste("FDR:", fdr))
# print(paste("C-index:", cindex_test))
# print(brier_scores)
# print(paste("Integrated Brier Score (IBS):", ibs))
}
results[[sim]] <- sim_results
}, error = function(e) {
cat("Error in simulation", sim, ":", conditionMessage(e), "\n")
})
}
print(results)
average_results <- list()
for (model_name in names(models)) {
model_metrics <- list(
recall = rep(NA, n_simulations),
accuracy = rep(NA, n_simulations),
f1_score = rep(NA, n_simulations),
fdr = rep(NA, n_simulations),
fnr = rep(NA, n_simulations),
cindex_test = rep(NA, n_simulations),
ibs = rep(NA, n_simulations),
brier_scores = vector("list", n_simulations)
)
for (sim in 1:n_simulations) {
if (!is.null(results[[sim]]) && model_name %in% names(results[[sim]])) {
model_metrics$recall[sim] <- results[[sim]][[model_name]]$recall
model_metrics$accuracy[sim] <- results[[sim]][[model_name]]$accuracy
model_metrics$f1_score[sim] <- results[[sim]][[model_name]]$f1_score
model_metrics$fdr[sim] <- results[[sim]][[model_name]]$fdr
model_metrics$fnr[sim] <- results[[sim]][[model_name]]$fnr
model_metrics$cindex_test[sim] <- results[[sim]][[model_name]]$cindex_test
model_metrics$ibs[sim] <- results[[sim]][[model_name]]$ibs
model_metrics$brier_scores[[sim]] <- results[[sim]][[model_name]]$brier_scores
}
}
average_results[[model_name]] <- list(
recall = mean(model_metrics$recall, na.rm = TRUE),
accuracy = mean(model_metrics$accuracy, na.rm = TRUE),
f1_score = mean(model_metrics$f1_score, na.rm = TRUE),
fdr = mean(model_metrics$fdr, na.rm = TRUE),
fnr = mean(model_metrics$fnr, na.rm = TRUE),
cindex_test = mean(model_metrics$cindex_test, na.rm = TRUE),
ibs = mean(model_metrics$ibs, na.rm = TRUE),
brier_scores = if (length(Filter(Negate(is.null), model_metrics$brier_scores)) > 0) {
Reduce("+", Filter(Negate(is.null), model_metrics$brier_scores)) / length(Filter(Negate(is.null), model_metrics$brier_scores))
} else {
NA
}
)
}
# Print average results
print(average_results)
# Convert average_results into a data frame
average_results_df <- data.frame(
Metric = c("Recall", "Accuracy", "F1 Score", "FDR","FNR", "C-index", "Brier Score", "IBS"),
ExclusiveLasso = c(average_results$ExclusiveLasso$recall,
average_results$ExclusiveLasso$accuracy,
average_results$ExclusiveLasso$f1_score,
average_results$ExclusiveLasso$fdr,
average_results$ExclusiveLasso$fnr,
average_results$ExclusiveLasso$cindex_test,
mean(unlist(average_results$ExclusiveLasso$brier_scores)),
average_results$ExclusiveLasso$ibs),
Lasso = c(average_results$Lasso$recall,
average_results$Lasso$accuracy,
average_results$Lasso$f1_score,
average_results$Lasso$fdr,
average_results$Lasso$fnr,
average_results$Lasso$cindex_test,
mean(unlist(average_results$Lasso$brier_scores)),
average_results$Lasso$ibs),
GroupLasso = c(average_results$GroupLasso$recall,
average_results$GroupLasso$accuracy,
average_results$GroupLasso$f1_score,
average_results$GroupLasso$fdr,
average_results$GroupLasso$fnr,
average_results$GroupLasso$cindex_test,
mean(unlist(average_results$GroupLasso$brier_scores)),
average_results$GroupLasso$ibs),
IPF = c(average_results$IPF$recall,
average_results$IPF$accuracy,
average_results$IPF$f1_score,
average_results$IPF$fdr,
average_results$IPF$fnr,
average_results$IPF$cindex_test,
mean(unlist(average_results$IPF$brier_scores)),
average_results$IPF$ibs),
PriorityLasso = c(average_results$PriorityLasso$recall,
average_results$PriorityLasso$accuracy,
average_results$PriorityLasso$f1_score,
average_results$PriorityLasso$fdr,
average_results$PriorityLasso$fnr,
average_results$PriorityLasso$cindex_test,
mean(unlist(average_results$PriorityLasso$brier_scores)),
average_results$PriorityLasso$ibs)
)
# Print the table
print(average_results_df)
library(ExclusiveLasso)
library(survival)
library(glmnet)
library(grpreg)
library(survcomp)
library(survcompare)
library(ipflasso)
library(prioritylasso)
library(MASS)
n_simulations <- 2
# Set parameters
n <- 1000          # Number of observations
p <- 500         # Number of variables
groups <- 5       # Number of groups
vars_per_group <- p / groups  # Variables per group
a <- 0.6       # Correlation within group
b <- 0.3         # Correlation between groups
lambda <- log(2) / 8  # Baseline hazard for median survival of 8 years
#set.seed(123)     # For reproducibility
# Construct the Toeplitz covariance matrix
Sigma <- matrix(0, nrow = p, ncol = p)
for (i in 1:p) {
for (j in 1:p) {
if (ceiling(i / vars_per_group) == ceiling(j / vars_per_group)) {
Sigma[i, j] <- a^abs(i - j)  # Within-group correlation
} else {
Sigma[i, j] <- b^abs(i - j)  # Between-group correlation
}
}
}
######################################################
#######################################################
results <- list()
for (sim in 1:n_simulations) {
cat("\nSimulation", sim, "...\n")
tryCatch({
# Generate Data
# Generate centered and scaled Gaussian data
X <- mvrnorm(n = n, mu = rep(0, p), Sigma = Sigma)
# Select one true variable per group
true_vars <- seq(1, p, by = vars_per_group)
beta <- rep(0, p)
beta[true_vars] <- runif(groups, -0.05, 1.15)  # Nonzero effects for true predictors
# Generate survival times from an exponential distribution
linear_predictor <- X %*% beta
hazard <- lambda * exp(linear_predictor)
survival_time <- rexp(n, rate = hazard)
# Generate independent censoring times from U(2,6)
censoring_time <- runif(n, min = 2, max = 6)
# Observed event times and censoring indicator
event_time <- pmin(survival_time, censoring_time)
status <- as.numeric(survival_time <= censoring_time)  # 1 = event, 0 = censored
# Create survival dataset
surv_data <- data.frame(time = event_time, status = status, X)
# Split data into training and testing sets
#set.seed(12345)  # Ensure reproducibility for the split
train_indices <- sample(1:n, size = 500, replace = FALSE)  # 50 training samples
test_indices <- setdiff(1:n, train_indices)
# Combine into a data frame
simulated_data <- data.frame(
time = event_time,
status = status,
X
)
training_data <- simulated_data[train_indices, ]
testing_data <- simulated_data[test_indices, ]
y<-Surv(simulated_data$time,simulated_data$status)
# Split the dataset into features (X) and survival outcome (T and E)
X_train <- as.matrix(training_data[, setdiff(names(training_data), c("time", "status"))])
y_train <- Surv(training_data$time, training_data$status)
X_test <- as.matrix(testing_data[, setdiff(names(testing_data), c("time", "status"))])
y_test <- Surv(testing_data$time, testing_data$status)
group <- rep(1:groups, each = vars_per_group)
sum(beta!=0)
true_coefs <- beta
group_size <- vars_per_group
n_groups <- groups
# IPF Model Setup
groupi <- rep(group_size, n_groups)
group_boundaries <- cumsum(c(0, groupi))
df_list_train <- list(x = X_train, time = as.integer(y_train[,1]), status = as.integer(y_train[,2]))
df_list_validate <- list(x = X_test, time = as.integer(y_test[,1]), status = as.integer(y_test[,2]))
df_list_train$time <- ifelse(df_list_train$time <= 0, 0.00001, df_list_train$time)
group_indices <- lapply(seq_along(groupi), function(i) seq(group_boundaries[i] + 1, group_boundaries[i + 1]))
group_matrices <- lapply(group_indices, function(indices) X[, indices])
mean_coefs <- sapply(group_matrices, function(X_group) {
fit <- glmnet(X_group, y, family = "cox", alpha = 0)
coef <- fit$beta[, which.min(fit$lambda)]
mean(coef)
})
# IPF Model Setup
ranked_indices <- order(-abs(mean_coefs))  # Negative sign for descending order
new_blocks.list <- setNames(vector("list", length(ranked_indices)), paste0("bp", seq_along(ranked_indices)))
for (i in seq_along(ranked_indices)) {
group_number <- ranked_indices[i]  # Get the new group index
start_index <- group_boundaries[group_number] + 1
end_index <- group_boundaries[group_number + 1]
new_blocks.list[[i]] <- start_index:end_index  # Assign correct variable indices
}
# Define models
models <- list(
"ExclusiveLasso" = function() {
tryCatch({
cvfit <- cv.exclusive_lasso(X_train, y_train, groups = group, nfolds = 10, family = 'cox', type.measure = "cindex")
fit <- exclusive_lasso(X_train, y = y_train, lmd = cvfit$lmd.min, groups = group, family = "cox")
pred_test <- predict(fit, newx = X_test, s = cvfit$lmd.min, type = "link")
list(pred_test, coef(fit)[-1])
}, error = function(e) {
cat("Error in ExclusiveLasso:", conditionMessage(e), "\n")
return(NULL)
})
},
"Lasso" = function() {
tryCatch({
cvfit <- cv.glmnet(X_train, y_train, family = "cox",alpha = 0.5)
fit <- glmnet(X_train, y_train, family = "cox", alpha = 0.5, lambda = cvfit$lambda.min)
pred_test <- predict(fit, newx = X_test, s = cvfit$lambda.min, type = "link")
list(pred_test, coef(fit))
}, error = function(e) {
cat("Error in Lasso:", conditionMessage(e), "\n")
return(NULL)
})
},
# "ElasticNet" = function(alpha_value = 0.5) {  # Set default alpha to 0.5
#   tryCatch({
#     cvfit <- cv.glmnet(X_train, y_train, family = "cox", alpha = alpha_value)
#     fit <- glmnet(X_train, y_train, family = "cox", alpha = alpha_value, lambda = cvfit$lambda.min)
#     pred_test <- predict(fit, newx = X_test, s = cvfit$lambda.min, type = "link")
#     list(pred_test, coef(fit))
#   }, error = function(e) {
#     cat("Error in ElasticNet:", conditionMessage(e), "\n")
#     return(NULL)
#   })
# },
"GroupLasso" = function() {
tryCatch({
cvfit <- cv.grpsurv(X_train, y_train, group = group, penalty = "grLasso", alpha = 1)
fit <- grpsurv(X_train, y_train, group = group, penalty = "grLasso", alpha = 1, lambda = cvfit$lambda.min)
fit1 <- grpsurv(X_train, y_train, group = group, penalty = "grLasso", alpha = 1)
pred_test <- predict(fit1, X = X_test, type = "link", lambda = cvfit$lambda.min)
list(pred_test, coef(fit))
}, error = function(e) {
cat("Error in GroupLasso:", conditionMessage(e), "\n")
return(NULL)
})
},
"IPF" = function() {
tryCatch({
cvipf_fit <- cvr.ipflasso(X = df_list_train$x, Y = Surv(df_list_train$time, df_list_train$status),
family = "cox", blocks = group_indices, pf = abs(1 / mean_coefs),
nfolds = 5, ncv = 10, type.measure = "partial likelihood")
pred_test <- ipflasso.predict(object = cvipf_fit, Xtest = X_test)
list(pred_test, cvipf_fit$coeff[-1, cvipf_fit$ind.bestlambda])
}, error = function(e) {
cat("Error in IPF:", conditionMessage(e), "\n")
return(NULL)
})
},
"PriorityLasso" = function() {
tryCatch({
fit_pl <-prioritylasso(X=X_train,Y=y_train,family="cox", type.measure = "deviance",blocks=new_blocks.list,lambda.type = "lambda.min")
pred_pl_test <- predict(fit_pl, newx = X_test, type="link")
list(pred_pl_test, fit_pl$coefficients)
}, error = function(e) {
cat("Error in PriorityLasso:", conditionMessage(e), "\n")
return(NULL)
})
}
)
# Store results
sim_results <- list()
for (model_name in names(models)) {
#model_name<-"IPF"
res <- models[[model_name]]()
#pred_test <- res[[1]]
coeffs <- res[[2]]
if (model_name == "IPF") {
pred_test <- res[[1]]$linpredtest  # IPF-specific calculation
} else {
pred_test <- res[[1]] # Default survival probability calculation
}
#elasso_coefficients <- coef(fit_elasso)
#as.numeric(coeffs != 0)
selected_binary <- rep(0, length(coeffs))
non_zero_indices <- which(coeffs != 0)
selected_binary[non_zero_indices] <- 1
true_positives <- sum(true_coefs & selected_binary)
false_positives <- sum(selected_binary & !true_coefs)
false_negatives <- sum(true_coefs & !selected_binary)
true_negatives <- sum(!true_coefs & !selected_binary)
precision <- true_positives / (true_positives + false_positives)
recall <- true_positives / (true_positives + false_negatives)
f1_score <- ifelse((precision + recall) > 0, 2 * (precision * recall) / (precision + recall), 0)
fnr <- false_negatives / (false_negatives + true_positives)
fdr <- false_positives / (false_positives + true_positives)
accuracy <- (true_positives + true_negatives) / length(true_coefs)      # Compute recall
cindex_test <- concordance.index(pred_test, y_test[,1], y_test[,2])$c.index
# Compute Brier Score
surv_train <- exp(-pred_test)  # Default survival probability calculation
surv_validate <- exp(-pred_test)
# Define time points for Brier score calculation
time_points <- quantile(testing_data$time, probs = c(0.25, 0.50, 0.75))
y_predicted_newdata <- as.vector(surv_validate)
# Rename the "status" column to "event"
colnames(training_data)[colnames(training_data) == "status"] <- "event"
colnames(testing_data)[colnames(testing_data) == "status"] <- "event"
# Initialize Brier Score storage
brier_scores <- numeric(length(time_points))
names(brier_scores) <- paste0("Time_", time_points)
# Compute Brier Score
for (j in seq_along(time_points)) {
brier_scores[j] <- survcompare:::surv_brierscore(
y_predicted_newdata = y_predicted_newdata,
df_brier_train = training_data,
df_newdata = testing_data,
time_point = time_points[j],
weighted = FALSE
)
}
# Compute Integrated Brier Score (IBS)
time_points <- sort(time_points)
brier_scores <- na.omit(brier_scores)
time_points <- time_points[!is.na(brier_scores)]
if (length(brier_scores) > 1) {
time_intervals <- diff(time_points)
ibs <- sum(time_intervals * (brier_scores[-length(brier_scores)] + brier_scores[-1]) / 2) /
(max(time_points) - min(time_points))
} else {
ibs <- NA
}
sim_results[[model_name]] <- list(
recall = recall,
brier_scores = brier_scores,
ibs = ibs
)
sim_results[[model_name]] <- list(
recall = recall,
accuracy = accuracy,
f1_score = f1_score,
fdr = fdr,
fnr= fnr,
cindex_test = cindex_test,
brier_scores = brier_scores,
ibs = ibs
)
# print(paste("Model:", model_name))
# print(paste("Recall:", recall))
# print(paste("Accuracy:", accuracy))
# print(paste("F1 Score:", f1_score))
# print(paste("FDR:", fdr))
# print(paste("C-index:", cindex_test))
# print(brier_scores)
# print(paste("Integrated Brier Score (IBS):", ibs))
}
results[[sim]] <- sim_results
}, error = function(e) {
cat("Error in simulation", sim, ":", conditionMessage(e), "\n")
})
}
print(results)
average_results <- list()
for (model_name in names(models)) {
model_metrics <- list(
recall = rep(NA, n_simulations),
accuracy = rep(NA, n_simulations),
f1_score = rep(NA, n_simulations),
fdr = rep(NA, n_simulations),
fnr = rep(NA, n_simulations),
cindex_test = rep(NA, n_simulations),
ibs = rep(NA, n_simulations),
brier_scores = vector("list", n_simulations)
)
for (sim in 1:n_simulations) {
if (!is.null(results[[sim]]) && model_name %in% names(results[[sim]])) {
model_metrics$recall[sim] <- results[[sim]][[model_name]]$recall
model_metrics$accuracy[sim] <- results[[sim]][[model_name]]$accuracy
model_metrics$f1_score[sim] <- results[[sim]][[model_name]]$f1_score
model_metrics$fdr[sim] <- results[[sim]][[model_name]]$fdr
model_metrics$fnr[sim] <- results[[sim]][[model_name]]$fnr
model_metrics$cindex_test[sim] <- results[[sim]][[model_name]]$cindex_test
model_metrics$ibs[sim] <- results[[sim]][[model_name]]$ibs
model_metrics$brier_scores[[sim]] <- results[[sim]][[model_name]]$brier_scores
}
}
average_results[[model_name]] <- list(
recall = mean(model_metrics$recall, na.rm = TRUE),
accuracy = mean(model_metrics$accuracy, na.rm = TRUE),
f1_score = mean(model_metrics$f1_score, na.rm = TRUE),
fdr = mean(model_metrics$fdr, na.rm = TRUE),
fnr = mean(model_metrics$fnr, na.rm = TRUE),
cindex_test = mean(model_metrics$cindex_test, na.rm = TRUE),
ibs = mean(model_metrics$ibs, na.rm = TRUE),
brier_scores = if (length(Filter(Negate(is.null), model_metrics$brier_scores)) > 0) {
Reduce("+", Filter(Negate(is.null), model_metrics$brier_scores)) / length(Filter(Negate(is.null), model_metrics$brier_scores))
} else {
NA
}
)
}
# Print average results
print(average_results)
# Convert average_results into a data frame
average_results_df <- data.frame(
Metric = c("Recall", "Accuracy", "F1 Score", "FDR","FNR", "C-index", "Brier Score", "IBS"),
ExclusiveLasso = c(average_results$ExclusiveLasso$recall,
average_results$ExclusiveLasso$accuracy,
average_results$ExclusiveLasso$f1_score,
average_results$ExclusiveLasso$fdr,
average_results$ExclusiveLasso$fnr,
average_results$ExclusiveLasso$cindex_test,
mean(unlist(average_results$ExclusiveLasso$brier_scores)),
average_results$ExclusiveLasso$ibs),
Lasso = c(average_results$Lasso$recall,
average_results$Lasso$accuracy,
average_results$Lasso$f1_score,
average_results$Lasso$fdr,
average_results$Lasso$fnr,
average_results$Lasso$cindex_test,
mean(unlist(average_results$Lasso$brier_scores)),
average_results$Lasso$ibs),
GroupLasso = c(average_results$GroupLasso$recall,
average_results$GroupLasso$accuracy,
average_results$GroupLasso$f1_score,
average_results$GroupLasso$fdr,
average_results$GroupLasso$fnr,
average_results$GroupLasso$cindex_test,
mean(unlist(average_results$GroupLasso$brier_scores)),
average_results$GroupLasso$ibs),
IPF = c(average_results$IPF$recall,
average_results$IPF$accuracy,
average_results$IPF$f1_score,
average_results$IPF$fdr,
average_results$IPF$fnr,
average_results$IPF$cindex_test,
mean(unlist(average_results$IPF$brier_scores)),
average_results$IPF$ibs),
PriorityLasso = c(average_results$PriorityLasso$recall,
average_results$PriorityLasso$accuracy,
average_results$PriorityLasso$f1_score,
average_results$PriorityLasso$fdr,
average_results$PriorityLasso$fnr,
average_results$PriorityLasso$cindex_test,
mean(unlist(average_results$PriorityLasso$brier_scores)),
average_results$PriorityLasso$ibs)
)
# Print the table
print(average_results_df)
